{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN) for Atari Pong\\n",
    "\\n",
    "In this notebook, we will implement a Deep Q-Network (DQN) to play the Atari game \\"Pong\\". We will use PyTorch to build the Q-network and OpenAI Gym to simulate the game environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari,accept-rom-license]==0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\\n",
    "import random\\n",
    "import numpy as np\\n",
    "import torch\\n",
    "import torch.nn as nn\\n",
    "import torch.optim as optim\\n",
    "import torch.nn.functional as F\\n",
    "from collections import deque\\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Q-Network\\n",
    "\\n",
    "We will use a simple convolutional neural network (CNN) as our Q-network. The network will take a stack of four consecutive game frames as input and output the Q-values for each possible action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\\n",
    "    def __init__(self, num_actions):\\n",
    "        super(QNetwork, self).__init__()\\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\\n",
    "        self.fc2 = nn.Linear(512, num_actions)\\n",
    "\\n",
    "    def forward(self, x):\\n",
    "        x = F.relu(self.conv1(x))\\n",
    "        x = F.relu(self.conv2(x))\\n",
    "        x = F.relu(self.conv3(x))\\n",
    "        x = x.view(x.size(0), -1)\\n",
    "        x = F.relu(self.fc1(x))\\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experience Replay\\n",
    "\\n",
    "We will use a `deque` to store the agent's experiences. The `ReplayBuffer` class will provide a method to sample a mini-batch of experiences from the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\\n",
    "    def __init__(self, capacity):\\n",
    "        self.buffer = deque(maxlen=capacity)\\n",
    "\\n",
    "    def push(self, state, action, reward, next_state, done):\\n",
    "        self.buffer.append((state, action, reward, next_state, done))\\n",
    "\\n",
    "    def sample(self, batch_size):\\n",
    "        return random.sample(self.buffer, batch_size)\\n",
    "\\n",
    "    def __len__(self):\\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The DQN Agent\\n",
    "\\n",
    "The `DQNAgent` class will encapsulate the Q-network, the target network, the replay buffer, and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\\n",
    "    def __init__(self, env, learning_rate=1e-4, gamma=0.99, buffer_size=10000, batch_size=32):\\n",
    "        self.env = env\\n",
    "        self.learning_rate = learning_rate\\n",
    "        self.gamma = gamma\\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\\n",
    "        self.batch_size = batch_size\\n",
    "\\n",
    "        self.q_network = QNetwork(env.action_space.n)\\n",
    "        self.target_network = QNetwork(env.action_space.n)\\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\\n",
    "\\n",
    "    def select_action(self, state, epsilon):\\n",
    "        if random.random() < epsilon:\\n",
    "            return self.env.action_space.sample()\\n",
    "        else:\\n",
    "            with torch.no_grad():\\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\\n",
    "                q_values = self.q_network(state)\\n",
    "                return q_values.argmax().item()\\n",
    "\\n",
    "    def update_model(self):\\n",
    "        if len(self.replay_buffer) < self.batch_size:\\n",
    "            return\\n",
    "\\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\\n",
    "\\n",
    "        state_batch = torch.FloatTensor(np.array(state_batch))\\n",
    "        action_batch = torch.LongTensor(action_batch)\\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\\n",
    "        next_state_batch = torch.FloatTensor(np.array(next_state_batch))\\n",
    "        done_batch = torch.FloatTensor(done_batch)\\n",
    "\\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\\n",
    "        next_q_values = self.target_network(next_state_batch).max(1)[0]\\n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\\n",
    "\\n",
    "        loss = F.mse_loss(q_values, expected_q_values.detach())\\n",
    "\\n",
    "        self.optimizer.zero_grad()\\n",
    "        loss.backward()\\n",
    "        self.optimizer.step()\\n",
    "\\n",
    "    def update_target_network(self):\\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\\n",
    "\\n",
    "We need to preprocess the game frames before feeding them to the Q-network. This includes converting the frames to grayscale and stacking four consecutive frames together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\\n",
    "\\n",
    "def preprocess_env(env):\\n",
    "    env = ResizeObservation(env, (84, 84))\\n",
    "    env = GrayScaleObservation(env)\\n",
    "    env = FrameStack(env, 4)\\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\\n",
    "\\n",
    "Now we can create the environment and the DQN agent and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, num_episodes=1000, max_steps=10000, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=200):\\n",
    "    rewards = []\\n",
    "    for episode in range(num_episodes):\\n",
    "        state, _ = agent.env.reset()\\n",
    "        episode_reward = 0\\n",
    "        for step in range(max_steps):\\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\\n",
    "            action = agent.select_action(np.array(state), epsilon)\\n",
    "            next_state, reward, done, _, _ = agent.env.step(action)\\n",
    "            agent.replay_buffer.push(np.array(state), action, reward, np.array(next_state), done)\\n",
    "            agent.update_model()\\n",
    "            state = next_state\\n",
    "            episode_reward += reward\\n",
    "            if done:\\n",
    "                break\\n",
    "        rewards.append(episode_reward)\\n",
    "        if episode % 10 == 0:\\n",
    "            agent.update_target_network()\\n",
    "            print(f\\"Episode {episode}, Reward: {episode_reward}\\")\\n",
    "    return rewards\\n",
    "\\n",
    "if __name__ == '__main__':\\n",
    "    env = gym.make('PongNoFrameskip-v4')\\n",
    "    env = preprocess_env(env)\\n",
    "    agent = DQNAgent(env)\\n",
    "    rewards = train(agent)\\n",
    "\\n",
    "    plt.plot(rewards)\\n",
    "    plt.xlabel('Episode')\\n",
    "    plt.ylabel('Reward')\\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
